{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Crawling Script for Games Writer Kirk Hamilton ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: import all necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pprint\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: download a sample webpage. \n",
    "You can save the html page onto your computer and use text editor to view its content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = requests.get('https://kotaku.com/detroit-become-human-the-kotaku-review-1826277408')\n",
    "#URL Options:\n",
    "#https://kotaku.com/detroit-become-human-the-kotaku-review-1826277408\n",
    "#https://kotaku.com/hollow-knight-the-kotaku-review-1827367425\n",
    "#https://kotaku.com/destiny-2-the-kotaku-review-1818530629\n",
    "#https://kotaku.com/no-mans-sky-the-kotaku-review-1785383774"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: use BeautifulSoup to parse the webpage and extract the lyrics content. The division that includes the lyrics starts from the html tag \"lyrics-body-text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(url.content, 'html.parser')\n",
    "title = soup.title.string\n",
    "print(title)\n",
    "\n",
    "divTag = soup.findAll('div',attrs={\"class\":\"post-content entry-content js_entry-content \"})\n",
    "body = []\n",
    "for tag in divTag:\n",
    "    for element in tag.findAll(\"p\"):\n",
    "        ptext = element.text\n",
    "        body.append(ptext)\n",
    "\n",
    "print(body)\n",
    "body = str(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: split text into individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = body.split()\n",
    "words = [element.lower() for element in words]\n",
    "#print(words)\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or use NLTK's tokenizer to split text into words. For more details about NLTK, read the documentation at http://www.nltk.org/book/ch05.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "tokens = nltk.word_tokenize(body)\n",
    "#print(tokens)\n",
    "tags = nltk.pos_tag(tokens)\n",
    "#print(tags)\n",
    "print(tags[0][0], tags[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "def removeStopwords(wordlist, stopwords):\n",
    "  return [w for w in wordlist if w not in stopwords]\n",
    "words = removeStopwords(words, stopwords)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = dict()\n",
    "for word in words:\n",
    "  counts[word] = counts.get(word,0) + 1\n",
    "sorted(counts, key=counts.__getitem__, reverse=True)\n",
    "pprint.pprint(counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sort words by frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above method uses loop, which needs quite a lot of programming, and is also slow. \n",
    "The following method uses the dataframe data structure in the pandas package to quickly count and sort words by frequencies. \n",
    "Pandas documentation includes more details on its powerful data structure\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(words, columns=['word'])\n",
    "x=df[\"word\"].value_counts()\n",
    "pprint.pprint(x)\n",
    "filename_pfx = title.split(' ', 1)[0]\n",
    "x.to_csv('results.csv', sep = \",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
